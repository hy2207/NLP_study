{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d0d1ca",
   "metadata": {},
   "source": [
    "### 토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0bf4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6925c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word_tokenize:  ['Do', \"n't\", 'forget', 'to', 'bring', 'Choi', \"'s\", 'pen']\n",
      "using WordPunctTokenizer:  ['Don', \"'\", 't', 'forget', 'to', 'bring', 'Choi', \"'\", 's', 'pen']\n",
      "using text_to_word_sequence:  [\"don't\", 'forget', 'to', 'bring', \"choi's\", 'pen']\n"
     ]
    }
   ],
   "source": [
    "input_words = \"Don't forget to bring Choi's pen\"\n",
    "\n",
    "# 방법1) word_tokenize 사용\n",
    "w1 = word_tokenize(input_words) \n",
    "print(\"using word_tokenize: \", w1)\n",
    "\n",
    "# 방법2) wordPunctTokenizer 사용\n",
    "w2 = WordPunctTokenizer().tokenize(input_words)\n",
    "print(\"using WordPunctTokenizer: \", w2)\n",
    "\n",
    "# 방법3) text_to_word_sequence 사용\n",
    "w3 = text_to_word_sequence(input_words)\n",
    "print(\"using text_to_word_sequence: \", w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ed10c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  ['Hi', ',', 'I', \"'m\", 'a', 'student', 'and', '18-year-old.', 'It', \"'s\", 'beautiful', 'day', 'today', '!']\n"
     ]
    }
   ],
   "source": [
    "#표준토큰화 사용\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Hi, I'm a student and 18-year-old. It's beautiful day today!\"\n",
    "t1 = tokenizer.tokenize(text)\n",
    "\n",
    "print(\"result: \", t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e51953e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  [\"e.g stands for 'exempli gratia', meaning 'for example'.\", 'This is used to provide specific examples that fall under a more general category.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"e.g stands for 'exempli gratia', meaning 'for example'. \\\n",
    "        This is used to provide specific examples that fall under a more general category.\"\n",
    "print(\"result: \", sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b505e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40eb2a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 :  ['IP 192.196.60.3 서버에서 확인 가능해.', '로그 파일을 aaa@naver.com으로 보내줘.']\n"
     ]
    }
   ],
   "source": [
    "text =\"IP 192.196.60.3 서버에서 확인 가능해. 로그 파일을 aaa@naver.com으로 보내줘.\"\n",
    "print(\"결과 : \", kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e9f24b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/chy/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b956eabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 :  ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'I', \"'m\", 'glad', 'to', 'know', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "품사 태깅 :  [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('I', 'PRP'), (\"'m\", 'VBP'), ('glad', 'JJ'), ('to', 'TO'), ('know', 'VB'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 품사태깅 예제\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"I am actively looking for Ph.D. students. I'm glad to know you are a Ph.D. student.\"\n",
    "result = word_tokenize(text)\n",
    "\n",
    "print('토큰화 결과 : ', result)\n",
    "print('품사 태깅 : ', pos_tag(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bd4a7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석 :  ['IP', '192.196', '.', '60.3', '서버', '에서', '확인', '가능해', '.', '로그', '파일', '을', 'aaa@naver.com', '으로', '보내줘', '.']\n",
      "OKT 품사 태깅 :  [('IP', 'Alpha'), ('192.196', 'Number'), ('.', 'Punctuation'), ('60.3', 'Number'), ('서버', 'Noun'), ('에서', 'Josa'), ('확인', 'Noun'), ('가능해', 'Adjective'), ('.', 'Punctuation'), ('로그', 'Noun'), ('파일', 'Noun'), ('을', 'Josa'), ('aaa@naver.com', 'Email'), ('으로', 'Josa'), ('보내줘', 'Verb'), ('.', 'Punctuation')]\n",
      "OKT 명사 추출 :  ['서버', '확인', '로그', '파일']\n"
     ]
    }
   ],
   "source": [
    "# 한국어 품사 태깅 예제\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "text =\"IP 192.196.60.3 서버에서 확인 가능해. 로그 파일을 aaa@naver.com으로 보내줘.\"\n",
    "\n",
    "print('OKT 형태소 분석 : ', okt.morphs(text))\n",
    "print('OKT 품사 태깅 : ', okt.pos(text))\n",
    "print('OKT 명사 추출 : ', okt.nouns(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
