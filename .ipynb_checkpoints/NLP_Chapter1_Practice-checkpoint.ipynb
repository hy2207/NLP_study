{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4fd57ec",
   "metadata": {},
   "source": [
    "## 1. 표준 토근화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5c6a6",
   "metadata": {},
   "source": [
    "표준 토큰화 중 하나인 Treebank 표준 토큰화를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ace09bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'cuntion', 'for', 'the', 'policy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Model-based RL don't need a value cuntion for the policy.\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a017c7f2",
   "metadata": {},
   "source": [
    "> don't 단어가 do 와 n't(not)으로 분리되는 것이 확인이 가능.\n",
    "또한 period 도 하나의 토큰으로 분리됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ce971",
   "metadata": {},
   "source": [
    "### 1-2. 토큰화 라이브러리\n",
    "Treebank 토큰화 이외에도 NLTK 패키지에는 여러종류의 토큰화 패키지가 존재함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6109a82a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model-based', 'RL', 'do', \"n't\", 'need', 'a', 'value', 'cuntion', 'for', 'the', 'policy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84782713",
   "metadata": {},
   "source": [
    "> 이 예제에서는 결과가 같지만 향후 사용에서는 좀 더 맞는 토큰화 패키지를 사용하면 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3236a0",
   "metadata": {},
   "source": [
    "## 2. 어간 추출 및 표제어 추출\n",
    "2-1. 어간추출 (stemmer) vs. 표제어 추출(Lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "777a630e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer :  ['eat', 'ate', 'eaten', 'eat']\n",
      "Lancaster Stemmer :  ['eat', 'at', 'eat', 'eat']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "stem1 = PorterStemmer()\n",
    "stem2 = LancasterStemmer()\n",
    "words = [\"eat\", \"ate\", \"eaten\", \"eating\"]\n",
    "print(\"Porter Stemmer : \", [stem1.stem(w) for w in words])\n",
    "print(\"Lancaster Stemmer : \", [stem2.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619ffc5",
   "metadata": {},
   "source": [
    "> 대표적 어간 추출인 Porter 및 Lancaster 추출 패키지를 불러오고 활용 </br>\n",
    "예제로는 \"먹다(eat) 단어를 사용 </br>\n",
    "문제점? stemmer의 특징마다 결과가 다르게 나옴 lancaster경우 at으로,,?</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3582e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/chy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet Lemmatizer:  ['eat', 'eat', 'eat', 'eat']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemm = WordNetLemmatizer()\n",
    "print(\"WordNet Lemmatizer: \", [lemm.lemmatize(w, pos=\"v\") for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454cfb2",
   "metadata": {},
   "source": [
    "> lemmatize 할 때 \"동사\"라고 지정을 해줘서 시제를 동사원형으로 시켜주는 기능. </br>\n",
    "품사를 알고있다면 Lemmatize 가 더 유용하게 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c3814",
   "metadata": {},
   "source": [
    "## 3. 불용어 (stopword) 제거\n",
    "3-1. 예시 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a647d320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/chy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english')[:5]) #예시 5개만 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b88da6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'should', 'all', 'study', 'hard', 'for', 'the', 'exam', '.']\n",
      "['We', 'study', 'hard', 'exam', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/chy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "input_sentence = \"We should all study hard for the exam.\"\n",
    "stop_words = set(stopwords.words('english')) #불용어 목록\n",
    "word_tokens = word_tokenize(input_sentence) #토큰화\n",
    "result = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words: #불용어 목록에 없는 경우\n",
    "        result.append(w)\n",
    "print(word_tokens)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a3f18",
   "metadata": {},
   "source": [
    "## 4. 정수 Encoding 및 Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba45155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cup', 8), ('July', 6), ('piano', 4), ('apple', 2), ('orange', 1)]\n",
      "{'cup': 1, 'July': 2, 'piano': 3, 'apple': 4, 'orange': 5}\n"
     ]
    }
   ],
   "source": [
    "vocab = {'apple':2, 'July': 6, 'piano': 4, 'cup': 8, 'orange': 1}\n",
    "vocab_sort = sorted(vocab.items(), key = lambda x: x[1], reverse = True) #내림차순정렬\n",
    "print(vocab_sort)\n",
    "word2inx = {word[0] : index + 1 for index, word in enumerate(vocab_sort)} #정수 인코딩 결과\n",
    "print(word2inx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8ab78",
   "metadata": {},
   "source": [
    "> **Bag of words (Bow)**: 단어마다 등장 빈도수를 나타냄 </br>\n",
    "cf) lambda: 임시함수로 위 예제에서는 key 값을 vocab에서 value 값으로 사용하기 위해 임시로 설정함 </br>\n",
    "*word2inx* 에 빈도수가 많은 단어 순으로 index를 설정하고 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e51914ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model-based': 0, 'RL': 1, 'do': 2, \"n't\": 3, 'need': 4, 'a': 5, 'value': 6, 'function': 7, 'for': 8, 'the': 9, 'policy': 10, ',': 11, 'but': 12, 'some': 13, 'of': 14, 'algorithms': 15, 'have': 16, '.': 17}\n",
      "[2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenize = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Model-based RL don't need a value function for the policy, \"\\\n",
    "        \"but some of Model-based RL algorithms do have a value function.\"\n",
    "\n",
    "token_text = tokenizer.tokenize(text) #토큰화\n",
    "word2inx = {}\n",
    "Bow = []\n",
    "\n",
    "for word in token_text:\n",
    "    if word not in word2inx.keys():\n",
    "        word2inx[word] = len(word2inx) #토큰화된 단어마다 index를 설정\n",
    "        Bow.insert(len(word2inx)-1, 1)\n",
    "    else:\n",
    "        inx = word2inx.get(word)\n",
    "        Bow[inx] += 1\n",
    "print(word2inx)\n",
    "print(Bow) #각 토큰의 빈도수로 이루어진 리스트 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c56203",
   "metadata": {},
   "source": [
    "## 5. 유사도 분석\n",
    "5-1. 코사인 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a9e3fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.7071067811865475 0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cos_sim(A, B):\n",
    "    return np.dot(A,B) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "\n",
    "a = [1, 0, 0, 1]\n",
    "b = [0, 1, 1, 0]\n",
    "c = [1, 1, 1, 1]\n",
    "print(cos_sim(a,b), cos_sim(b,c), cos_sim(c,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589167e",
   "metadata": {},
   "source": [
    "> 1에 가까울 수록 유사도가 높음을 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f204637",
   "metadata": {},
   "source": [
    "5-2. 레반슈타인 거리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f59df49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 2. 3. 4. 5.]\n",
      " [1. 0. 1. 2. 3. 4.]\n",
      " [2. 1. 0. 1. 2. 3.]\n",
      " [3. 2. 1. 1. 2. 3.]\n",
      " [4. 3. 2. 2. 1. 2.]\n",
      " [5. 4. 3. 3. 2. 2.]\n",
      " [6. 5. 4. 4. 3. 2.]]\n"
     ]
    }
   ],
   "source": [
    "def leven(text1, text2):\n",
    "    len1 = len(text1) + 1\n",
    "    len2 = len(text2) + 1\n",
    "    sim_array = np.zeros((len1, len2)) #0으로 이루어진 행렬생성\n",
    "    sim_array[:, 0] = np.linspace(0, len1-1, len1) #첫번째 column에 0부터 1간격으로 숫자채움\n",
    "    sim_array[0, :] = np.linspace(0, len2-1, len2) #row에 0부터 1간격으로 숫자채움\n",
    "    for i in range(1, len1):\n",
    "        for j in range(1, len2):\n",
    "            add_char = sim_array[i-1, j] + 1 #추가\n",
    "            sub_char = sim_array[i, j-1] + 1 #삭제\n",
    "            if text1[i-1] == text2[j-1]:\n",
    "                mod_char = sim_array[i-1, j-1] #수정\n",
    "            else:\n",
    "                mod_char = sim_array[i-1, j-1] + 1\n",
    "            sim_array[i, j] = min([add_char, sub_char, mod_char]) #값 비교해서 최소값으로 저장\n",
    "    return sim_array\n",
    "#     return sim_array[-1, -1] #가장오른쪽 끝값을 가져오므로 레반슈타인의 거리를 구할 수 있음\n",
    "print(leven('데이터마이닝', '데이타마닝'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
